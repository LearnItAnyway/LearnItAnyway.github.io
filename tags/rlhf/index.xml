<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rlhf on LIA&#39;log</title>
    <link>https://learnitanyway.github.io/tags/rlhf/</link>
    <description>Recent content in rlhf on LIA&#39;log</description>
    <image>
      <title>LIA&#39;log</title>
      <url>https://learnitanyway.github.io/images/papermod-cover.png</url>
      <link>https://learnitanyway.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 24 Jan 2024 11:07:52 +0900</lastBuildDate>
    <atom:link href="https://learnitanyway.github.io/tags/rlhf/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[scrap, sketch] Preference Optimization for Diffusion Model</title>
      <link>https://learnitanyway.github.io/posts/scrap/po_for_diffusion/</link>
      <pubDate>Wed, 24 Jan 2024 11:07:52 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/po_for_diffusion/</guid>
      <description>I&amp;rsquo;v write some equations of DPO and CPO. Recently, DPO for the diffusion model has been studied, which shows the better performance for the SDXL. The DPO-Diffusion loss is given as $$ \begin{align} \mathcal{L} \leq -\log \sigma \left(\beta T \log\frac{\pi_ \theta(x_ 1^ {t-1}|x_1^ t)}{\pi_ \text{ref}(x_ 1^ {t-1}|x_1^ t)} - \beta T \log\frac{\pi_ \theta(x_ 2^ {t-1}|x_1^ t)}{\pi_ \text{ref}(x_ 2^ {t-1}|x_1^ t)}\right) \end{align} $$ where $1$ is index for the preferred data and $2$ is that for the rejected data.</description>
    </item>
    <item>
      <title>[scrap] CPO - contrastive preference optimization </title>
      <link>https://learnitanyway.github.io/posts/scrap/cpo/</link>
      <pubDate>Tue, 23 Jan 2024 16:19:43 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/cpo/</guid>
      <description>Introduction Contrastive preference optimization (CPO) is methods to train the text generating LM model from human preference dataset. It takes advantages of low memory cunsumption for the training and less overfitting. The loss of DPO is given as
$$ \begin{align} \mathcal{L}_ \text{DPO} &amp;amp;= -\log (p^ * (y_ 1&amp;gt;y_ 2 | x)) \\ &amp;amp;= -\log \sigma \left(\beta\log\left(\frac{\pi(y_1|x)}{\pi_\text{ref}(y_1|x)}\right) - \beta\log\left(\frac{\pi(y_2|x)}{\pi_\text{ref}(y_2|x)}\right)\right) \end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$.</description>
    </item>
    <item>
      <title>[scrap] DPO - direct preference optimization </title>
      <link>https://learnitanyway.github.io/posts/scrap/dpo/</link>
      <pubDate>Sun, 14 Jan 2024 12:53:10 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/dpo/</guid>
      <description>Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as
$$ \begin{align} p^* (y_1&amp;gt;y_2|x) = \frac{\exp(r^* (x, y_1))}{\exp(r^* (x, y_1))+\exp(r^* (x, y_2))} \end{align} $$
Simply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.</description>
    </item>
  </channel>
</rss>
