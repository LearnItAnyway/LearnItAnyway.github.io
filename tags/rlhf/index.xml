<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rlhf on LIA&#39;log</title>
    <link>https://learnitanyway.github.io/tags/rlhf/</link>
    <description>Recent content in rlhf on LIA&#39;log</description>
    <image>
      <title>LIA&#39;log</title>
      <url>https://learnitanyway.github.io/images/papermod-cover.png</url>
      <link>https://learnitanyway.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 14 Jan 2024 12:53:10 +0900</lastBuildDate>
    <atom:link href="https://learnitanyway.github.io/tags/rlhf/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[scrap] DPO - direct preference optimization </title>
      <link>https://learnitanyway.github.io/posts/scrap/dpo/</link>
      <pubDate>Sun, 14 Jan 2024 12:53:10 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/dpo/</guid>
      <description>Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as
$$ \begin{align} p^* (y_1&amp;gt;y_2|x) = \frac{\exp(r^* (x, y_1))}{\exp(r^* (x, y_1))+\exp(r^* (x, y_2))} \end{align} $$
Simply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.</description>
    </item>
  </channel>
</rss>
