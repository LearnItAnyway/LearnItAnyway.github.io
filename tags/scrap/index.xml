<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scrap on LIA&#39;log</title>
    <link>https://learnitanyway.github.io/tags/scrap/</link>
    <description>Recent content in scrap on LIA&#39;log</description>
    <image>
      <title>LIA&#39;log</title>
      <url>https://learnitanyway.github.io/images/papermod-cover.png</url>
      <link>https://learnitanyway.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 04 Feb 2024 12:47:47 +0900</lastBuildDate>
    <atom:link href="https://learnitanyway.github.io/tags/scrap/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[scrap] KVQuant - KV Cache Quantization for Long Context LLM</title>
      <link>https://learnitanyway.github.io/posts/scrap/kvquant/</link>
      <pubDate>Sun, 04 Feb 2024 12:47:47 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/kvquant/</guid>
      <description>Intro KVQuant is</description>
    </item>
    <item>
      <title>[scrap] CPO - contrastive preference optimization </title>
      <link>https://learnitanyway.github.io/posts/scrap/cpo/</link>
      <pubDate>Tue, 23 Jan 2024 16:19:43 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/cpo/</guid>
      <description>Introduction Contrastive preference optimization (CPO) is methods to train the text generating LM model from human preference dataset. It takes advantages of low memory cunsumption for the training and less overfitting. The loss of DPO is given as
$$ \begin{align} \mathcal{L}_ \text{DPO} &amp;amp;= -\log (p^ * (y_ 1&amp;gt;y_ 2 | x)) \\ &amp;amp;= -\log \sigma \left(\beta\log\left(\frac{\pi(y_1|x)}{\pi_\text{ref}(y_1|x)}\right) - \beta\log\left(\frac{\pi(y_2|x)}{\pi_\text{ref}(y_2|x)}\right)\right) \end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$.</description>
    </item>
    <item>
      <title>[scrap] Decision Transformer</title>
      <link>https://learnitanyway.github.io/posts/scrap/decision_former/</link>
      <pubDate>Thu, 18 Jan 2024 20:55:26 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/decision_former/</guid>
      <description>Intro and Method Decision transformer is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.
For the inputs of the transformers, state, action, and returns-to-go $\hat R_ t= \Sigma_ {t&amp;rsquo;=t} ^ {T} r_ {t&amp;rsquo;}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision. The Algorithm is given as below.
Experiments DT vs BC on a subset of data?</description>
    </item>
    <item>
      <title>[scrap] DPO - direct preference optimization </title>
      <link>https://learnitanyway.github.io/posts/scrap/dpo/</link>
      <pubDate>Sun, 14 Jan 2024 12:53:10 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/dpo/</guid>
      <description>Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as
$$ \begin{align} p^* (y_1&amp;gt;y_2|x) = \frac{\exp(r^* (x, y_1))}{\exp(r^* (x, y_1))+\exp(r^* (x, y_2))} \end{align} $$
Simply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.</description>
    </item>
    <item>
      <title>[scrap] Encodec</title>
      <link>https://learnitanyway.github.io/posts/scrap/encodec/</link>
      <pubDate>Sat, 13 Jan 2024 12:47:47 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/encodec/</guid>
      <description>Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for
Audio compression Discrete audio token prediction as in VALL-E or Bark </description>
    </item>
  </channel>
</rss>
