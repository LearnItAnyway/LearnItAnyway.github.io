<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
	<script>
	MathJax = {
	  tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	  },
	  startup: {
		ready: () => {
		  MathJax.startup.defaultReady();
		  document.querySelectorAll("script[type='math/tex']").forEach((node) => {
			MathJax.tex2chtmlPromise(node.textContent, {display: false}).then((html) => {
			  node.parentNode.replaceChild(html, node);
			});
		  });
		}
	  }
	};
	</script>	

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>[scrap] Decision Transformer | LIA&#39;log</title>
<meta name="keywords" content="RL, IL, scrap">
<meta name="description" content="Intro and Method Decision transformer is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.
For the inputs of the transformers, state, action, and returns-to-go $\hat R_ t= \Sigma_ {t&rsquo;=t} ^ {T} r_ {t&rsquo;}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision. The Algorithm is given as below.
Experiments DT vs BC on a subset of data?">
<meta name="author" content="">
<link rel="canonical" href="https://learnitanyway.github.io/posts/scrap/decision_former/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://learnitanyway.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://learnitanyway.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://learnitanyway.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://learnitanyway.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://learnitanyway.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="[scrap] Decision Transformer" />
<meta property="og:description" content="Intro and Method Decision transformer is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.
For the inputs of the transformers, state, action, and returns-to-go $\hat R_ t= \Sigma_ {t&rsquo;=t} ^ {T} r_ {t&rsquo;}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision. The Algorithm is given as below.
Experiments DT vs BC on a subset of data?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://learnitanyway.github.io/posts/scrap/decision_former/" /><meta property="og:image" content="https://learnitanyway.github.io/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-18T20:55:26+09:00" />
<meta property="article:modified_time" content="2024-01-18T20:55:26+09:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://learnitanyway.github.io/images/papermod-cover.png"/>

<meta name="twitter:title" content="[scrap] Decision Transformer"/>
<meta name="twitter:description" content="Intro and Method Decision transformer is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.
For the inputs of the transformers, state, action, and returns-to-go $\hat R_ t= \Sigma_ {t&rsquo;=t} ^ {T} r_ {t&rsquo;}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision. The Algorithm is given as below.
Experiments DT vs BC on a subset of data?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://learnitanyway.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "scrap",
      "item": "https://learnitanyway.github.io/posts/scrap/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "[scrap] Decision Transformer",
      "item": "https://learnitanyway.github.io/posts/scrap/decision_former/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[scrap] Decision Transformer",
  "name": "[scrap] Decision Transformer",
  "description": "Intro and Method Decision transformer is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.\nFor the inputs of the transformers, state, action, and returns-to-go $\\hat R_ t= \\Sigma_ {t\u0026rsquo;=t} ^ {T} r_ {t\u0026rsquo;}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision. The Algorithm is given as below.\nExperiments DT vs BC on a subset of data?",
  "keywords": [
    "RL", "IL", "scrap"
  ],
  "articleBody": "Intro and Method Decision transformer is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.\nFor the inputs of the transformers, state, action, and returns-to-go $\\hat R_ t= \\Sigma_ {t’=t} ^ {T} r_ {t’}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision. The Algorithm is given as below.\nExperiments DT vs BC on a subset of data? Percentile BC, BC on only top X% of timesteps in the dataset DT is stronger when the data in the low data regimes Ability of DT to understand return-to-go return-to-go depends on the type of task and the length of the episodes E.g., larger episode length $T$ gives the larger return-to-go DT trains for specific episode length can perform well in different target return (different episode length) Extrapolation w.r.t. the return-to-go, implies the DT can understand (and generalized) across different return-to-go Effect of context length DT with context shows better performance than DT without context $K=50$ for Pong, and $K=30$ for others But the context length is related with the resources for the training and execution Long-term credit assignment and sparse reward DT is strong at long-term credit assignment in both RL and %BC DT can trained with sparse reward Other.. DT does not need policy regularization or conservatism DT takes returns-to-go as input and do not need critic $\\rightarrow$ no policy regularziation or something Benefit over other offline RL Sample efficient and poses ability to generaize over different objectives ",
  "wordCount" : "254",
  "inLanguage": "en",
  "datePublished": "2024-01-18T20:55:26+09:00",
  "dateModified": "2024-01-18T20:55:26+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://learnitanyway.github.io/posts/scrap/decision_former/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LIA'log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://learnitanyway.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://learnitanyway.github.io/" accesskey="h" title="LIA&#39;log (Alt + H)">LIA&#39;log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://learnitanyway.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/LearnItAnyway" title="Github">
                    <span>Github</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://learnitanyway.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://learnitanyway.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://learnitanyway.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://learnitanyway.github.io/posts/scrap/">scrap</a></div>
    <h1 class="post-title entry-hint-parent">
      [scrap] Decision Transformer
    </h1>
    <div class="post-meta"><span title='2024-01-18 20:55:26 +0900 KST'>January 18, 2024</span>&nbsp;·&nbsp;2 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intro-and-method" aria-label="Intro and Method">Intro and Method</a></li>
                <li>
                    <a href="#experiments" aria-label="Experiments">Experiments</a><ul>
                        <ul>
                        
                <li>
                    <a href="#dt-vs-bc-on-a-subset-of-data" aria-label="DT vs BC on a subset of data?">DT vs BC on a subset of data?</a></li>
                <li>
                    <a href="#ability-of-dt-to-understand-return-to-go" aria-label="Ability of DT to understand return-to-go">Ability of DT to understand return-to-go</a></li>
                <li>
                    <a href="#effect-of-context-length" aria-label="Effect of context length">Effect of context length</a></li>
                <li>
                    <a href="#long-term-credit-assignment-and-sparse-reward" aria-label="Long-term credit assignment and sparse reward">Long-term credit assignment and sparse reward</a></li>
                <li>
                    <a href="#other" aria-label="Other..">Other..</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="intro-and-method">Intro and Method<a hidden class="anchor" aria-hidden="true" href="#intro-and-method">#</a></h1>
<p><a href="https://arxiv.org/pdf/2106.01345.pdf">Decision transformer</a> is the RL method that use causal transformer to output the action using the sequences of state, action, and reward.</p>
<p><img loading="lazy" src="dt_1.png" alt=""  />
<img loading="lazy" src="/dt_1.png" alt=""  />

For the inputs of the transformers, state, action, and returns-to-go $\hat R_ t= \Sigma_ {t&rsquo;=t} ^ {T} r_ {t&rsquo;}$. Perhaps, we can skip returns-to-go if the reward is not diretly obtainable during the decision.
The Algorithm is given as below.</p>
<p><img loading="lazy" src="dt_2.png" alt=""  />
<img loading="lazy" src="/dt_2.png" alt=""  />
</p>
<h1 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h1>
<p><img loading="lazy" src="dt_3.png" alt=""  />
<img loading="lazy" src="/dt_3.png" alt=""  />
</p>
<h3 id="dt-vs-bc-on-a-subset-of-data">DT vs BC on a subset of data?<a hidden class="anchor" aria-hidden="true" href="#dt-vs-bc-on-a-subset-of-data">#</a></h3>
<ul>
<li>Percentile BC, BC on only top X% of timesteps in the dataset</li>
<li>DT is stronger when the data in the low data regimes
<img loading="lazy" src="dt_4.png" alt=""  />
<img loading="lazy" src="/dt_4.png" alt=""  />
</li>
</ul>
<h3 id="ability-of-dt-to-understand-return-to-go">Ability of DT to understand return-to-go<a hidden class="anchor" aria-hidden="true" href="#ability-of-dt-to-understand-return-to-go">#</a></h3>
<ul>
<li>return-to-go depends on the type of task and the length of the episodes
<ul>
<li>E.g., larger episode length $T$ gives the larger return-to-go</li>
</ul>
</li>
<li>DT trains for specific episode length can perform well in different target return (different episode length)
<ul>
<li>Extrapolation w.r.t. the return-to-go, implies the DT can understand (and generalized) across different return-to-go
<img loading="lazy" src="dt_5.png" alt=""  />
<img loading="lazy" src="/dt_5.png" alt=""  />
</li>
</ul>
</li>
</ul>
<h3 id="effect-of-context-length">Effect of context length<a hidden class="anchor" aria-hidden="true" href="#effect-of-context-length">#</a></h3>
<ul>
<li>DT with context shows better performance than DT without context
<ul>
<li>$K=50$ for Pong, and $K=30$ for others</li>
</ul>
</li>
<li>But the context length is related with the resources for the training and execution</li>
</ul>
<p><img loading="lazy" src="dt_6.png" alt=""  />
<img loading="lazy" src="/dt_6.png" alt=""  />
</p>
<h3 id="long-term-credit-assignment-and-sparse-reward">Long-term credit assignment and sparse reward<a hidden class="anchor" aria-hidden="true" href="#long-term-credit-assignment-and-sparse-reward">#</a></h3>
<ul>
<li>DT is strong at long-term credit assignment in both RL and %BC</li>
<li>DT can trained with sparse reward
<img loading="lazy" src="dt_7.png" alt=""  />
<img loading="lazy" src="/dt_7.png" alt=""  />
</li>
</ul>
<h3 id="other">Other..<a hidden class="anchor" aria-hidden="true" href="#other">#</a></h3>
<ul>
<li>DT does not need policy regularization or conservatism
<ul>
<li>DT takes returns-to-go as input and do not need critic $\rightarrow$ no policy regularziation or something</li>
</ul>
</li>
<li>Benefit over other offline RL
<ul>
<li>Sample efficient and poses  ability to generaize over different objectives</li>
</ul>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://learnitanyway.github.io/tags/rl/">RL</a></li>
      <li><a href="https://learnitanyway.github.io/tags/il/">IL</a></li>
      <li><a href="https://learnitanyway.github.io/tags/scrap/">scrap</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://learnitanyway.github.io/posts/scrap/cpo/">
    <span class="title">« Prev</span>
    <br>
    <span>[scrap] CPO - contrastive preference optimization </span>
  </a>
  <a class="next" href="https://learnitanyway.github.io/posts/scrap/rt-2/">
    <span class="title">Next »</span>
    <br>
    <span>[scrap] Robotic Transformer 2</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://learnitanyway.github.io/">LIA&#39;log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
