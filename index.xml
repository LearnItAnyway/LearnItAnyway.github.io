<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LIA&#39;log</title>
    <link>https://learnitanyway.github.io/</link>
    <description>Recent content on LIA&#39;log</description>
    <image>
      <title>LIA&#39;log</title>
      <url>https://learnitanyway.github.io/images/papermod-cover.png</url>
      <link>https://learnitanyway.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 13 Jan 2024 15:21:05 +0900</lastBuildDate>
    <atom:link href="https://learnitanyway.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[scrap] Robotic Transformer 2</title>
      <link>https://learnitanyway.github.io/posts/scrap/rt-2/</link>
      <pubDate>Thu, 18 Jan 2024 20:55:20 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/rt-2/</guid>
      <description>Introduction Robotic Transformer 2 (RT-2) is the vision-language-action model for the robotic control trained with the imitation learning. The pretrained vision-language models are fine tuned with robotic control dataset and vision-language dataset from web have been used, where both the language response and robotic actions are the same format. The use of pretrained visual-language model enables the semantic aware policies, i.e., if the task is interpretable through the visual-language model, it can perform even though the task is not included in the trained data.</description>
    </item>
    <item>
      <title>[test] Difference between encodec and descript audio codec</title>
      <link>https://learnitanyway.github.io/posts/scrap/diff_in_encodec/</link>
      <pubDate>Tue, 16 Jan 2024 09:54:10 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/diff_in_encodec/</guid>
      <description>Motivation During Dec., 2023, I&amp;rsquo;v pretrain the VALL-E model for Korean. Specifically, the pretrained model is released at the huggingface, however, as the pronounciation of the model and voice cloning ability is somewhat low due to 2k hours (from one data source) of the training data. To improve further, a new model has been trained with 8k hours (from 3 data sources) of the data, however, it shows much lower performance compared with the prior model, even though the almost every setup are identical.</description>
    </item>
    <item>
      <title>[scrap] AWQ - activation-aware weight quantization</title>
      <link>https://learnitanyway.github.io/posts/scrap/awq/</link>
      <pubDate>Mon, 15 Jan 2024 11:28:06 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/awq/</guid>
      <description>Introduction Quantization methods map a floating poitn number into the integer. E.g., float 16 can be maps to the int 4 with a certain amount of quantization error, where both the memory usage and inference speed can be improved. Among them, activation-aware weight quantization (AWQ) scale the weight considering the activation of the group of values.
Method Motivation of activation-aware weight quantization is that
Some weight are more salient than the other Considering the activation not the weight, the introducing some scaler can be improved The quantization function in AWQ is defined as</description>
    </item>
    <item>
      <title>[scrap] DPO - direct preference optimization </title>
      <link>https://learnitanyway.github.io/posts/scrap/dpo/</link>
      <pubDate>Sun, 14 Jan 2024 12:53:10 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/dpo/</guid>
      <description>Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as
$$ \begin{align} p^* (y_1&amp;gt;y_2|x) = \frac{\exp(r^* (x, y_1))}{\exp(r^* (x, y_1))+\exp(r^* (x, y_2))} \end{align} $$
Simply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.</description>
    </item>
    <item>
      <title>[scrap] Encodec</title>
      <link>https://learnitanyway.github.io/posts/scrap/encodec/</link>
      <pubDate>Sat, 13 Jan 2024 12:47:47 +0900</pubDate>
      <guid>https://learnitanyway.github.io/posts/scrap/encodec/</guid>
      <description>Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for
Audio compression Discrete audio token prediction as in VALL-E or Bark </description>
    </item>
  </channel>
</rss>
