[{"content":"Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as\n$$ \\begin{align} p^* (y_1\u0026gt;y_2|x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1))+\\exp(r^* (x, y_2))} \\end{align} $$\nSimply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.\n$$ \\max_{\\pi_\\theta} \\mathbb{E}_ {x \\sim\\mathcal{D}, y\\sim\\pi_{\\theta}(y|x)} [r_\\phi(x, y)] - \\beta \\mathbb{D}_ \\text{KL}[\\pi_{\\theta}(y|x)||\\pi_\\text{ref}(y|x)] $$\nHere, $\\pi_\\theta$ is the LM and $r_\\phi$ is is reward function (that should be trained). In RLHF, the KL divergence between the LM and reference LM is used as well. KL divergence prevent the LM from being too defferent compared to the reference LM. From some RL work, the optimal solution for the problem above is given as $$ \\pi(y|x) = \\frac{1}{Z(x)} \\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right), $$ where $Z(x)=\\sum_ y \\left(\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\\right)$ is the partition function. Take the log for the both side, the relationship between the optimal policy and the reward is given as $$ r(x, y) = \\beta\\log\\left(\\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}\\right)+\\beta\\log Z(x). $$ Using this reward function, the preference can be rewritten as $$ p^ * (y_1\u0026gt;y_2|x) = \\frac{1}{1+\\exp\\left(\\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right)\\right)} $$ Then, the objective is $$ \\begin{align} \\mathcal{L}_ \\text{DPO} \u0026amp;= -\\log (p^ * (y_ 1\u0026gt;y_ 2 | x)) \\\\ \u0026amp;= -\\log \\left(\\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right)\\right) \\end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$.\n","permalink":"https://learnitanyway.github.io/posts/scrap/dpo/","summary":"Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as\n$$ \\begin{align} p^* (y_1\u0026gt;y_2|x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1))+\\exp(r^* (x, y_2))} \\end{align} $$\nSimply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.","title":"[scrap] DPO"},{"content":"Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for\nAudio compression Discrete audio token prediction as in VALL-E or Bark ","permalink":"https://learnitanyway.github.io/posts/scrap/encodec/","summary":"Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for\nAudio compression Discrete audio token prediction as in VALL-E or Bark ","title":"[scrap] Encodec"}]