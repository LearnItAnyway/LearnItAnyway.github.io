[{"content":"I\u0026rsquo;v write some equations of DPO and CPO. Recently, DPO for the diffusion model has been studied, which shows the better performance for the SDXL. The DPO-Diffusion loss is given as $$ \\begin{align} \\mathcal{L} \\leq -\\log \\sigma \\left(\\beta T \\log\\frac{\\pi_ \\theta(x_ 1^ {t-1}|x_1^ t)}{\\pi_ \\text{ref}(x_ 1^ {t-1}|x_1^ t)} - \\beta T \\log\\frac{\\pi_ \\theta(x_ 2^ {t-1}|x_1^ t)}{\\pi_ \\text{ref}(x_ 2^ {t-1}|x_1^ t)}\\right) \\end{align} $$ where $1$ is index for the preferred data and $2$ is that for the rejected data. Superscript $t$ and $t-1$ means for the diffusion paths. The loss is given as $$ \\begin{align} \\mathcal{L}_ \\text{DPO-diff} \u0026amp;=-\\log \\sigma \\left[-\\beta T (\\mathbb{D}(\\pi_ \\theta, x_ 1, t)-\\mathbb{D}(\\pi_ \\text{ref}, x_ 1, t)-\\mathbb{D}(\\pi_ \\theta, x_ 2, t)+\\mathbb{D}(\\pi_ \\text{ref}, x_ 2, t)) \\right] \\\\ \u0026amp;=-\\log \\sigma \\left[-\\beta T w(\\lambda_t)(\\epsilon (\\pi_ \\theta, x_ 1, t)-\\epsilon(\\pi_ \\text{ref}, x_ 1, t)-\\epsilon(\\pi_ \\theta, x_ 2, t)+\\epsilon(\\pi_ \\text{ref}, x_ 2, t)) \\right] \\end{align} $$ where $$ \\begin{align} \\mathbb{D} (\\pi, x, t)\u0026amp;=\\mathbb{D}_ \\text{KL} (q(x^ {t-1}|x^ {0,t})||\\pi(x^ {t-1}|x^ t)) \\\\ \\epsilon(\\pi, x, t)\u0026amp;= ||\\epsilon - \\epsilon(x^ t, t) ||_ 2^ 2 \\end{align} $$ Here, $x_ t^ *=\\alpha_ t x_ 0^ * + \\sigma_ t \\epsilon^ *$, $\\epsilon^ * \\sim \\mathcal{N} (0, I)$ is drawn from $q$, $\\lambda_ t=\\alpha_ t^ 2/\\sigma_ t^ 2$ is signal to noise ratio, and $w(\\lambda _t)$ is the weighting function.\nSimilarly, CPO diffusion loss can be given by $$ \\begin{align} \\mathcal{L}_ \\text{CPO-diff} \u0026amp;= -\\log (-w(\\lambda_ t)\\epsilon(\\pi_ \\theta, x_ 1, t)) -\\log \\sigma \\left[-\\beta T w(\\lambda_t)(\\epsilon (\\pi_ \\theta, x_ 1, t)-\\epsilon(\\pi_ \\theta, x_ 2, t)) \\right] \\\\ \u0026amp;= \\mathcal{L}_ \\text{NLL-diff} + \\mathcal{L}_ \\text{preference-diff} \\end{align} $$ As CPO does, CPO diffusion loss takes advantages of lower memory consumption and computational resources as it does not need reference model. Need more experiments. (until now, 24.Jan , there is no paper or report about CPO diffusion loss.)\n","permalink":"https://learnitanyway.github.io/posts/scrap/po_for_diffusion/","summary":"I\u0026rsquo;v write some equations of DPO and CPO. Recently, DPO for the diffusion model has been studied, which shows the better performance for the SDXL. The DPO-Diffusion loss is given as $$ \\begin{align} \\mathcal{L} \\leq -\\log \\sigma \\left(\\beta T \\log\\frac{\\pi_ \\theta(x_ 1^ {t-1}|x_1^ t)}{\\pi_ \\text{ref}(x_ 1^ {t-1}|x_1^ t)} - \\beta T \\log\\frac{\\pi_ \\theta(x_ 2^ {t-1}|x_1^ t)}{\\pi_ \\text{ref}(x_ 2^ {t-1}|x_1^ t)}\\right) \\end{align} $$ where $1$ is index for the preferred data and $2$ is that for the rejected data.","title":"[scrap, sketch] Preference Optimization for Diffusion Model"},{"content":"Introduction Contrastive preference optimization (CPO) is methods to train the text generating LM model from human preference dataset. It takes advantages of low memory cunsumption for the training and less overfitting. The loss of DPO is given as\n$$ \\begin{align} \\mathcal{L}_ \\text{DPO} \u0026amp;= -\\log (p^ * (y_ 1\u0026gt;y_ 2 | x)) \\\\ \u0026amp;= -\\log \\sigma \\left(\\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right)\\right) \\end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$. In the CPO, instead of supervised fine-tuned model for the $\\pi_ \\text{ref}$, they assume the ideal model $\\pi_ w$, where $\\pi_ w(y_1)=1$ and $0\\leq \\pi_ w(y_2)\\leq 1$. Then the DPO loss is reformulated as $$ \\begin{align} \\mathcal{L}_ \\theta \u0026amp;= -\\log \\sigma \\left[\\beta(\\log\\pi_ \\theta (y_1|x) - \\log\\pi_ w(y_1|x)-\\log \\pi_ \\theta(y_2|x) + \\log \\pi_ w (y_2|x)) \\right] \\\\ \u0026amp;= -\\log \\sigma \\left[ \\beta(\\log\\pi_ \\theta (y_1|x) -\\log \\pi_ \\theta(y_2|x) + \\log \\pi_ w (y_2|x)) \\right] \\\\ \u0026amp;= -\\log \\left(1/(1+\\frac{\\pi_ \\theta(y_2|x)^ \\beta}{\\pi_ \\theta(y_1|x)^ \\beta \\pi_w (y_2|x)^ \\beta})\\right) \\\\ \u0026amp;= -\\log \\left(\\pi_ \\theta(y_1|x)^ \\beta \\pi_w (y_2|x)^ \\beta/(\\pi_ \\theta(y_1|x)^ \\beta \\pi_w (y_2|x)^ \\beta+\\pi_ \\theta(y_2|x)^ \\beta)\\right) \\\\ \\end{align} $$ Since $w$ is not optimized parameters, the preference loss is given as $$ \\begin{align} \\mathcal{L}_ \\theta \u0026amp;= -\\beta \\log \\pi_ \\theta(y_1|x) + \\log(\\pi_ \\theta(y_1|x)^ \\beta \\pi_ w(y_2|x)^ \\beta + \\pi_ \\theta(y_2|x)^ \\beta) \\\\ \u0026amp;\\leq-\\beta \\log \\pi_ \\theta(y_1|x) + \\log(\\pi_ \\theta(y_1|x)^ \\beta \\cdot 1 + \\pi_ \\theta(y_2|x)^ \\beta) \\\\ \u0026amp;= - \\log \\sigma (\\beta \\log \\pi_ \\theta(y_ 1|x)-\\beta \\log \\pi_ \\theta(y_ 2|x))=\\mathcal{L}_ \\text{prefer} \\end{align} $$ The author additionally gives a log-likelihood loss for the preferred data. $$ \\begin{align} \\mathcal{L}_ \\text{NLL}\u0026amp;=-\\log \\pi_ \\theta (y_ 1|x) \\\\ \\mathcal{L}_ \\text{CPO}\u0026amp;= \\mathcal{L}_ \\text{prefer}+ \\mathcal{L}_ \\text{NLL} \\\\ \\end{align} $$\n","permalink":"https://learnitanyway.github.io/posts/scrap/cpo/","summary":"Introduction Contrastive preference optimization (CPO) is methods to train the text generating LM model from human preference dataset. It takes advantages of low memory cunsumption for the training and less overfitting. The loss of DPO is given as\n$$ \\begin{align} \\mathcal{L}_ \\text{DPO} \u0026amp;= -\\log (p^ * (y_ 1\u0026gt;y_ 2 | x)) \\\\ \u0026amp;= -\\log \\sigma \\left(\\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right)\\right) \\end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$.","title":"[scrap] CPO - contrastive preference optimization "},{"content":"Introduction Robotic Transformer 2 (RT-2) is the vision-language-action model for the robotic control trained with the imitation learning. The pretrained vision-language models are fine tuned with robotic control dataset and vision-language dataset from web have been used, where both the language response and robotic actions are the same format. The use of pretrained visual-language model enables the semantic aware policies, i.e., if the task is interpretable through the visual-language model, it can perform even though the task is not included in the trained data. Furthermore use of the same format improve the generalization.\nMethod Pretrained Vision-language Model Structure of the vision-language mode is like PaLI and LLaVA. In the paper, PaLI-X and PaLM-E are used and the key idea is not the specific model but the general vision-language model. In those model, the images are processed through the vision model, where the output of the vision model are fed into the language model as embedding tokens. Concatenated with the text embeddings, vision embeddings are handled by the several encoding layers and decoding layers. PaLI use encoder-decoder structure and LLaVA use decoder-only structure.\nRobot actions Action space consists of 6 $\\Delta$pos/$\\Delta$rot, grip, and terminal. 6 continuous and 2 discrete action. The paper discretize continuous action into 256 uniform bins. So task can be discrete token prediction. Some key point is summarized as:\n\u0026ldquo;terminate $\\Delta p_ x,\\Delta p_ y,\\Delta p_ z,\\Delta r_ x,\\Delta r_ y,\\Delta r_ z,g$\u0026rdquo; can be tokenized as [1, 128, \u0026hellip; 127], 8 integers. The token is trained with symbol tuning. As most of the original tokens are not for the robotic tasks, during inference, the output is sampled within the valid tokens (e.g., 1~256 for pos and all range for the NLP tasks). Although the heavy models are used, with the multi-TPU cloud services, frequency of the inference is about 1~5 Hz for 55B and 5Hz for 5B model. Co-fine tuning on both the robotics and web- data helps the performance of robot actions Experiments The paper try to answer the following things:\nnew task emergent capability generalization over different parameters whether CoT improves Imitation learning with set of robotics tasks New tasks Environment with unseen object, background\u0026hellip; Emergent capability Generalization over different parameters (model and dataset) Dataset - from scratch \u0026lt; fine-tuning \u0026lt; co-fine-tuning (intersting) Model - the larger is the better CoT Chain of Thought works. Some other Thought Failure Case Understanding Cases\nGrasping objects by specific parts, such as the handle Novel motions beyond what was seen in the robot data, such as wiping with a towel or tool use Dexterous or precise motions, such as folding a towel Extended reasoning requiring multiple layers of indirection Guess the reason (not in the paper and more likely wrong.)\nVision model Lack of robotics dataset It requires video-language dataset not just vision-language model Language model Training Details Model - PaLI-X, PaLM-E - not sure it is opend.. Batch size 2048 with 80K gradient steps, RT-2-PaLI-X-55B (Guess) LLaVA 7B on A6000 takes ~1sec for each data \u0026ndash;\u0026gt; 10~30 mins per batch? Over 10K GPU hours for the fine-tuning\u0026hellip;? ","permalink":"https://learnitanyway.github.io/posts/scrap/rt-2/","summary":"Introduction Robotic Transformer 2 (RT-2) is the vision-language-action model for the robotic control trained with the imitation learning. The pretrained vision-language models are fine tuned with robotic control dataset and vision-language dataset from web have been used, where both the language response and robotic actions are the same format. The use of pretrained visual-language model enables the semantic aware policies, i.e., if the task is interpretable through the visual-language model, it can perform even though the task is not included in the trained data.","title":"[scrap] Robotic Transformer 2"},{"content":"Motivation During Dec., 2023, I\u0026rsquo;v pretrain the VALL-E model for Korean. Specifically, the pretrained model is released at the huggingface, however, as the pronounciation of the model and voice cloning ability is somewhat low due to 2k hours (from one data source) of the training data. To improve further, a new model has been trained with 8k hours (from 3 data sources) of the data, however, it shows much lower performance compared with the prior model, even though the almost every setup are identical.\nMy hypothesis on the failure is that trains on 8k hours failed because the different dataset results in incosistant audio tokens. Let say we have the same audio file. If the volum of the audio changed, the audio tokens will be changed as well. Although the 2k hours of data has similar average volumn, 8k hours of data have difference in audio noise, sampling rate, and volum as well, which results in the different discrete audio tokens.\nTo check this, I\u0026rsquo;v first heck the differences in the audio files $\\rightarrow$ have clear difference, even the average volum are different over 10dB. Then I check the differences in the discrete token with different volumns through this. Furthermore, I\u0026rsquo;v check the performance of other discrete audio token method (dac).\nSimilarity of all tokens Similarity of the first tokens As shown above, even the volum changes 2.5dB, the token similarity has been reduced (0.4 for Encodec, and 0.7 for dac) which potentially reduces the performance of discrete audio prediction in VALL-E or Bark.\nNote For the audio token generation, the discrete autio encoding can be used With audio volum scale, tokens can vary which can lower the performance of autio token generation Possible solutions for this problem is Use of audio encoding methods that robust over noise, volume scaling, \u0026hellip; Data agumentation (noise, volum\u0026hellip; ) Joint training of encoding model and generation model ","permalink":"https://learnitanyway.github.io/posts/scrap/diff_in_encodec/","summary":"Motivation During Dec., 2023, I\u0026rsquo;v pretrain the VALL-E model for Korean. Specifically, the pretrained model is released at the huggingface, however, as the pronounciation of the model and voice cloning ability is somewhat low due to 2k hours (from one data source) of the training data. To improve further, a new model has been trained with 8k hours (from 3 data sources) of the data, however, it shows much lower performance compared with the prior model, even though the almost every setup are identical.","title":"[test] Difference between encodec and descript audio codec"},{"content":"Introduction Quantization methods map a floating poitn number into the integer. E.g., float 16 can be maps to the int 4 with a certain amount of quantization error, where both the memory usage and inference speed can be improved. Among them, activation-aware weight quantization (AWQ) scale the weight considering the activation of the group of values.\nMethod Motivation of activation-aware weight quantization is that\nSome weight are more salient than the other Considering the activation not the weight, the introducing some scaler can be improved The quantization function in AWQ is defined as\n$$ \\begin{align} Q(\\textbf{w}) \u0026amp;= \\Delta \\cdot \\text{Round} (\\frac{\\textbf{w}}{\\Delta }), \\Delta = \\frac{\\max(|\\textbf{w}|)}{2^ {N-1}} \\\\ \\textbf{y} \u0026amp;= \\textbf{w}\\textbf{x} \\approx Q(\\textbf{w})\\textbf{x} \\end{align} $$ The idea is to reduce the upper part of the $\\Delta$, i.e., $\\max(|\\textbf{w}|)$. with the scaler $s$, the element of the weight $w$ is quantized as $$ \\begin{align} y\\approx Q(ws) \\frac{x}{s} \u0026amp;= \\Delta\u0026rsquo; \\cdot \\text{Round}\u0026rsquo; (\\frac{ws}{\\Delta})s^ {-1} \\\\ \\end{align} $$ With sufficient choThe quantization scaler $\\Delta\\approx \\Delta \u0026lsquo;$ for large portion of s, the quantization error will be $s^ {-1}$ which results in the better quantization.\nTo choose the scaler, the following problem can be solved $$ \\begin{align} \\text{argmin}_\\textbf{s} ||Q(\\textbf{W}\\cdot \\textbf{s})(\\textbf{s}^ {-1} \\cdot \\textbf{X}) - \\textbf{WX} || \\end{align} $$\nResults Performance - Higher performance compared with RTN and GPTQ\nSpeed - Higher speed compared with the RTN and GTPQ Also, the quantization speed is higher compared with the prior works.\n","permalink":"https://learnitanyway.github.io/posts/scrap/awq/","summary":"Introduction Quantization methods map a floating poitn number into the integer. E.g., float 16 can be maps to the int 4 with a certain amount of quantization error, where both the memory usage and inference speed can be improved. Among them, activation-aware weight quantization (AWQ) scale the weight considering the activation of the group of values.\nMethod Motivation of activation-aware weight quantization is that\nSome weight are more salient than the other Considering the activation not the weight, the introducing some scaler can be improved The quantization function in AWQ is defined as","title":"[scrap] AWQ - activation-aware weight quantization"},{"content":"Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as\n$$ \\begin{align} p^* (y_1\u0026gt;y_2|x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1))+\\exp(r^* (x, y_2))} \\end{align} $$\nSimply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.\n$$ \\max_{\\pi_\\theta} \\mathbb{E}_ {x \\sim\\mathcal{D}, y\\sim\\pi_{\\theta}(y|x)} [r_\\phi(x, y)] - \\beta \\mathbb{D}_ \\text{KL}[\\pi_{\\theta}(y|x)||\\pi_\\text{ref}(y|x)] $$\nHere, $\\pi_\\theta$ is the LM and $r_\\phi$ is is reward function (that should be trained). In RLHF, the KL divergence between the LM and reference LM is used as well. KL divergence prevent the LM from being too defferent compared to the reference LM. From some RL work, the optimal solution for the problem above is given as $$ \\pi(y|x) = \\frac{1}{Z(x)} \\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right), $$ where $Z(x)=\\sum_ y \\left(\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\\right)$ is the partition function. Take the log for the both side, the relationship between the optimal policy and the reward is given as $$ r(x, y) = \\beta\\log\\left(\\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}\\right)+\\beta\\log Z(x). $$ Using this reward function, the preference can be rewritten as $$ p^ * (y_1\u0026gt;y_2|x) = \\frac{1}{1+\\exp\\left(\\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right)\\right)} $$ Then, the objective is $$ \\begin{align} \\mathcal{L}_ \\text{DPO} \u0026amp;= -\\log (p^ * (y_ 1\u0026gt;y_ 2 | x)) \\\\ \u0026amp;= -\\log \\sigma \\left(\\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right)\\right) \\end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$.\n","permalink":"https://learnitanyway.github.io/posts/scrap/dpo/","summary":"Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as\n$$ \\begin{align} p^* (y_1\u0026gt;y_2|x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1))+\\exp(r^* (x, y_2))} \\end{align} $$\nSimply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.","title":"[scrap] DPO - direct preference optimization "},{"content":"Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for\nAudio compression Discrete audio token prediction as in VALL-E or Bark ","permalink":"https://learnitanyway.github.io/posts/scrap/encodec/","summary":"Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for\nAudio compression Discrete audio token prediction as in VALL-E or Bark ","title":"[scrap] Encodec"}]