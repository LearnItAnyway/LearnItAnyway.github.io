[{"content":"Motivation During Dec., 2023, I\u0026rsquo;v pretrain the VALL-E model for Korean. Specifically, the pretrained model is released at the huggingface, however, as the pronounciation of the model and voice cloning ability is somewhat low due to 2k hours (from one data source) of the training data. To improve further, a new model has been trained with 8k hours (from 3 data sources) of the data, however, it shows much lower performance compared with the prior model, even though the almost every setup are identical.\nMy hypothesis on the failure is that trains on 8k hours failed because the different dataset results in incosistant audio tokens. Let say we have the same audio file. If the volum of the audio changed, the audio tokens will be changed as well. Although the 2k hours of data has similar average volumn, 8k hours of data have difference in audio noise, sampling rate, and volum as well, which results in the different discrete audio tokens.\nTo check this, I\u0026rsquo;v first heck the differences in the audio files $\\rightarrow$ have clear difference, even the average volum are different over 10dB. Then I check the differences in the discrete token with different volumns through this. Furthermore, I\u0026rsquo;v check the performance of other discrete audio token method (dac).\nSimilarity of all tokens Similarity of the first tokens As shown above, even the volum changes 2.5dB, the token similarity has been reduced (0.4 for Encodec, and 0.7 for dac) which potentially reduces the performance of discrete audio prediction in VALL-E or Bark.\nNote For the audio token generation, the discrete autio encoding can be used With audio volum scale, tokens can vary which can lower the performance of autio token generation Possible solutions for this problem is Use of audio encoding methods that robust over noise, volume scaling, \u0026hellip; Data agumentation (noise, volum\u0026hellip; ) Joint training of encoding model and generation model ","permalink":"https://learnitanyway.github.io/posts/scrap/diff_in_encodec/","summary":"Motivation During Dec., 2023, I\u0026rsquo;v pretrain the VALL-E model for Korean. Specifically, the pretrained model is released at the huggingface, however, as the pronounciation of the model and voice cloning ability is somewhat low due to 2k hours (from one data source) of the training data. To improve further, a new model has been trained with 8k hours (from 3 data sources) of the data, however, it shows much lower performance compared with the prior model, even though the almost every setup are identical.","title":"[test] Difference between encodec and descript audio codec"},{"content":"Introduction Quantization methods map a floating poitn number into the integer. E.g., float 16 can be maps to the int 4 with a certain amount of quantization error, where both the memory usage and inference speed can be improved. Among them, activation-aware weight quantization (AWQ) scale the weight considering the activation of the group of values.\nMethod Motivation of activation-aware weight quantization is that\nSome weight are more salient than the other Considering the activation not the weight, the introducing some scaler can be improved The quantization function in AWQ is defined as\n$$ \\begin{align} Q(\\textbf{w}) \u0026amp;= \\Delta \\cdot \\text{Round} (\\frac{\\textbf{w}}{\\Delta }), \\Delta = \\frac{\\max(|\\textbf{w}|)}{2^ {N-1}} \\\\ \\textbf{y} \u0026amp;= \\textbf{w}\\textbf{x} \\approx Q(\\textbf{w})\\textbf{x} \\end{align} $$ The idea is to reduce the upper part of the $\\Delta$, i.e., $\\max(|\\textbf{w}|)$. with the scaler $s$, the element of the weight $w$ is quantized as $$ \\begin{align} y\\approx Q(ws) \\frac{x}{s} \u0026amp;= \\Delta\u0026rsquo; \\cdot \\text{Round}\u0026rsquo; (\\frac{ws}{\\Delta})s^ {-1} \\\\ \\end{align} $$ With sufficient choThe quantization scaler $\\Delta\\approx \\Delta \u0026lsquo;$ for large portion of s, the quantization error will be $s^ {-1}$ which results in the better quantization.\nTo choose the scaler, the following problem can be solved $$ \\begin{align} \\text{argmin}_\\textbf{s} ||Q(\\textbf{W}\\cdot \\textbf{s})(\\textbf{s}^ {-1} \\cdot \\textbf{X}) - \\textbf{WX} || \\end{align} $$\nResults Performance - Higher performance compared with RTN and GPTQ\nSpeed - Higher speed compared with the RTN and GTPQ Also, the quantization speed is higher compared with the prior works.\n","permalink":"https://learnitanyway.github.io/posts/scrap/awq/","summary":"Introduction Quantization methods map a floating poitn number into the integer. E.g., float 16 can be maps to the int 4 with a certain amount of quantization error, where both the memory usage and inference speed can be improved. Among them, activation-aware weight quantization (AWQ) scale the weight considering the activation of the group of values.\nMethod Motivation of activation-aware weight quantization is that\nSome weight are more salient than the other Considering the activation not the weight, the introducing some scaler can be improved The quantization function in AWQ is defined as","title":"[scrap] AWQ - activation-aware weight quantization"},{"content":"Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as\n$$ \\begin{align} p^* (y_1\u0026gt;y_2|x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1))+\\exp(r^* (x, y_2))} \\end{align} $$\nSimply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.\n$$ \\max_{\\pi_\\theta} \\mathbb{E}_ {x \\sim\\mathcal{D}, y\\sim\\pi_{\\theta}(y|x)} [r_\\phi(x, y)] - \\beta \\mathbb{D}_ \\text{KL}[\\pi_{\\theta}(y|x)||\\pi_\\text{ref}(y|x)] $$\nHere, $\\pi_\\theta$ is the LM and $r_\\phi$ is is reward function (that should be trained). In RLHF, the KL divergence between the LM and reference LM is used as well. KL divergence prevent the LM from being too defferent compared to the reference LM. From some RL work, the optimal solution for the problem above is given as $$ \\pi(y|x) = \\frac{1}{Z(x)} \\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right), $$ where $Z(x)=\\sum_ y \\left(\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\\right)$ is the partition function. Take the log for the both side, the relationship between the optimal policy and the reward is given as $$ r(x, y) = \\beta\\log\\left(\\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}\\right)+\\beta\\log Z(x). $$ Using this reward function, the preference can be rewritten as $$ p^ * (y_1\u0026gt;y_2|x) = \\frac{1}{1+\\exp\\left(\\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right)\\right)} $$ Then, the objective is $$ \\begin{align} \\mathcal{L}_ \\text{DPO} \u0026amp;= -\\log (p^ * (y_ 1\u0026gt;y_ 2 | x)) \\\\ \u0026amp;= -\\log \\left(\\beta\\log\\left(\\frac{\\pi(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right) - \\beta\\log\\left(\\frac{\\pi(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right)\\right) \\end{align} $$ DPO make the model to output the preferred data $y_ 1$ rather than rejected one $y_ 2$.\n","permalink":"https://learnitanyway.github.io/posts/scrap/dpo/","summary":"Introduction Direct preference optimization (DPO) is the methods to train the text generating LM model from the human preference dataset. The relationship between the preference and the reward is given from Bardley-Terry model as\n$$ \\begin{align} p^* (y_1\u0026gt;y_2|x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1))+\\exp(r^* (x, y_2))} \\end{align} $$\nSimply put, the better output (one that has the higher reward) is more likely to be preffered. Based on this reward, the LM can be trained to maximize the reward of the output.","title":"[scrap] DPO - direct preference optimization "},{"content":"Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for\nAudio compression Discrete audio token prediction as in VALL-E or Bark ","permalink":"https://learnitanyway.github.io/posts/scrap/encodec/","summary":"Summary Encodec is neural codec for the audio compression. The github code and the pretrained models are provided by meta. The overall structure is given as follows The model can be used for\nAudio compression Discrete audio token prediction as in VALL-E or Bark ","title":"[scrap] Encodec"}]